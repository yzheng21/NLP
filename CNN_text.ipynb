{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "Building prefix dict from the default dictionary ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dumping model to file cache C:\\Windows\\TEMP\\jieba.cache\n",
      "Loading model cost 1.065 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "import os  \n",
    "import numpy as np  \n",
    "np.random.seed(1337)  \n",
    "import jieba  #处理中文\n",
    "import nltk  #处理英文\n",
    "#from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from keras.preprocessing.text import Tokenizer  \n",
    "from keras.preprocessing.sequence import pad_sequences  \n",
    "from keras.utils.np_utils import to_categorical  \n",
    "from keras.layers import Dense, Input, Flatten  \n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding  \n",
    "from keras.models import Model  \n",
    "from keras.optimizers import *  \n",
    "from keras.models import Sequential  \n",
    "from keras.layers import Merge  \n",
    "import sys\n",
    "def make_word_set(words_file):\n",
    "    words_set = set()\n",
    "    with open(words_file, 'rb') as fp:\n",
    "        for line in fp.readlines():\n",
    "            word = line.strip().decode('utf-8')\n",
    "            if len(word)>0 and word not in words_set: # 去重\n",
    "                words_set.add(word)\n",
    "    return words_set\n",
    "\n",
    "\n",
    "# 文本处理，也就是样本生成过程\n",
    "def text_processing(folder_path, test_size=0.2):\n",
    "    folder_list = os.listdir(folder_path)\n",
    "    data_list = []\n",
    "    class_list = []\n",
    "    folder_id = 0\n",
    "    # 遍历文件夹\n",
    "    for folder in folder_list:\n",
    "        new_folder_path = os.path.join(folder_path, folder)\n",
    "        files = os.listdir(new_folder_path)\n",
    "        # 读取文件\n",
    "        j = 1\n",
    "        for file in files:\n",
    "            if j > 100:  # 怕内存爆掉，只取100个样本文件，你可以注释掉取完\n",
    "                break\n",
    "            with open(os.path.join(new_folder_path, file), 'rb') as fp:\n",
    "                raw = fp.read().decode('utf-8')\n",
    "            ## 是的，随处可见的jieba中文分词\n",
    "            #jieba.enable_parallel(4)  # 开启并行分词模式，参数为并行进程数，不支持windows\n",
    "            word_cut = jieba.cut(raw, cut_all=False)  # 精确模式，返回的结构是一个可迭代的genertor\n",
    "            word_list = list(word_cut)  # genertor转化为list，每个词unicode格式\n",
    "            #jieba.disable_parallel()  # 关闭并行分词模式\n",
    "\n",
    "            data_list.append(word_list)  # 训练集list\n",
    "            class_list.append(folder_id)\n",
    "            # 类别\n",
    "            j += 1\n",
    "        folder_id += 1\n",
    "\n",
    "    # 其实可以用sklearn自带的部分做\n",
    "    train_data_list, test_data_list, train_class_list, test_class_list = train_test_split(data_list, class_list, test_size=test_size)\n",
    "\n",
    "    all_words_dict = {}\n",
    "    for word_list in data_list:\n",
    "        words_dict = Counter([word for word in word_list])\n",
    "        all_words_dict = dict(Counter(all_words_dict)+words_dict)\n",
    "    # key函数利用词频进行降序排序\n",
    "    all_words_tuple_list = sorted(all_words_dict.items(), key=lambda f: f[1], reverse=True)  # 内建函数sorted参数需为list\n",
    "    all_words_list = list(zip(*all_words_tuple_list))[0]\n",
    "\n",
    "    return data_list, class_list, all_words_list, train_data_list, test_data_list, train_class_list, test_class_list\n",
    "\n",
    "\n",
    "def words_dict(all_words_list, deleteN, stopwords_set=set()):\n",
    "    # 选取特征词\n",
    "    feature_words = []\n",
    "    n = 1\n",
    "    for t in range(deleteN, len(all_words_list), 1):\n",
    "        if n > 1000:  # feature_words的维度1000\n",
    "            break\n",
    "\n",
    "        if not all_words_list[t].isdigit() and all_words_list[t] not in stopwords_set and 1 < len(\n",
    "                all_words_list[t]) < 5:\n",
    "            feature_words.append(all_words_list[t])\n",
    "            n += 1\n",
    "    return feature_words\n",
    "\n",
    "# 文本特征\n",
    "def text_features(data_list, train_data_list, test_data_list,  feature_words, flag='nltk'):\n",
    "    def text_features(text, feature_words):\n",
    "        text_words = set(text)\n",
    "        ## -----------------------------------------------------------------------------------\n",
    "        if flag == 'nltk':\n",
    "            ## nltk特征 dict\n",
    "            features = {word:1 if word in text_words else 0 for word in feature_words}\n",
    "        elif flag == 'sklearn':\n",
    "            ## sklearn特征 list\n",
    "            features = [word for word in feature_words if word in text_words]\n",
    "        else:\n",
    "            features = []\n",
    "        ## -----------------------------------------------------------------------------------\n",
    "        return features\n",
    "    data = [text_features(text, feature_words) for text in data_list]\n",
    "    train_feature_list = [text_features(text, feature_words) for text in train_data_list]\n",
    "    test_feature_list = [text_features(text, feature_words) for text in test_data_list]\n",
    "    return data, train_feature_list, test_feature_list\n",
    "\n",
    "\n",
    "print (\"start\")\n",
    "\n",
    "\n",
    "## 文本预处理\n",
    "folder_path = 'Database/SogouC/Sample'\n",
    "data_list, class_list, all_words_list, train_data_list, test_data_list, train_class_list, test_class_list = text_processing(folder_path, test_size=0.2)\n",
    "\n",
    "# 生成stopwords_set\n",
    "stopwords_file = 'stopwords_cn.txt'\n",
    "stopwords_set = make_word_set(stopwords_file)\n",
    "\n",
    "## 文本特征提取和分类\n",
    "# flag = 'nltk'\n",
    "flag = 'sklearn'\n",
    "test_accuracy_list = []\n",
    "deleteN = 20\n",
    "feature_words = words_dict(all_words_list, deleteN, stopwords_set)\n",
    "data, train_feature_list, test_feature_list = text_features(data_list, train_data_list, test_data_list, feature_words, flag)\n",
    "\n",
    "\n",
    "print(len(data))\n",
    "data_list = []\n",
    "pure_text = ' '\n",
    "for item in data:    \n",
    "    data_list.append(pure_text.join(item))\n",
    "\n",
    "print (\"finished\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "f = open('glove.6B.50d.txt','rb') # 读入50维的词向量文件，可以改成100维或者其他  \n",
    "for line in f:  \n",
    "    values = line.split()  \n",
    "    word = values[0]  \n",
    "    coefs = np.asarray(values[1:], dtype='float32')  \n",
    "    embeddings_index[word] = coefs  \n",
    "f.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "import keras.preprocessing.text as T\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=20000) #num_words:None或整数,处理的最大单词数量。少于此数的单词丢掉\n",
    "tokenizer.fit_on_texts(data_list)\n",
    "sequences = tokenizer.texts_to_sequences(data_list)\n",
    "word_index = tokenizer.word_index\n",
    "print(len(word_index))\n",
    "data = pad_sequences(sequences, maxlen=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (90, 1000)\n",
      "Shape of label tensor: (90, 9)\n"
     ]
    }
   ],
   "source": [
    "labels = to_categorical(np.asarray(class_list))\n",
    "print('Shape of data tensor:', data.shape)  \n",
    "print('Shape of label tensor:', labels.shape)  \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.4, random_state=42)\n",
    "# # split the data into a training set and a validation set,下面这段代码，主要是将数据集分为，训练集和测试集（英文原意是验证集，但是我略有改动代码）  \n",
    "# indices = np.arange(data.shape[0])  \n",
    "# np.random.shuffle(indices)  \n",
    "# data = data[indices]  \n",
    "# labels = labels[indices]  \n",
    "# nb_validation_samples = int(0.4 * data.shape[0])  \n",
    "  \n",
    "# x_train = data[:-nb_validation_samples] # 训练集  \n",
    "# y_train = labels[:-nb_validation_samples]# 训练集的标签  \n",
    "# x_val = data[-nb_validation_samples:] # 测试集，英文原意是验证集  \n",
    "# y_val = labels[-nb_validation_samples:] # 测试集的标签 \n",
    "# print(x_train.shape)\n",
    "# print(y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1001, 50)\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 1000 # 每个文本的最长选取长度，较短的文本可以设短些  \n",
    "MAX_NB_WORDS = 20000 # 整体词库字典中，词的多少，可以略微调大或调小  \n",
    "EMBEDDING_DIM = 50 # 词向量的维度，可以根据实际情况使用，如果不了解暂时不要改 \n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words + 1, EMBEDDING_DIM))  \n",
    "for word, i in word_index.items():  \n",
    "    if i > MAX_NB_WORDS:  \n",
    "        continue  \n",
    "    embedding_vector = embeddings_index.get(word)  \n",
    "    if embedding_vector is not None:  \n",
    "        # words not found in embedding index will be all-zeros.  \n",
    "        embedding_matrix[i] = embedding_vector # wor\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zheng\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:48: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    }
   ],
   "source": [
    "# 神经网路的第一层，词向量层，本文使用了预训练glove词向量，可以把trainable那里设为False  \n",
    "embedding_layer = Embedding(nb_words + 1,  \n",
    "                            EMBEDDING_DIM,  \n",
    "                            input_length=MAX_SEQUENCE_LENGTH,  \n",
    "                            weights=[embedding_matrix],  \n",
    "                            trainable=True)  \n",
    "  \n",
    "print('Training model.')  \n",
    "  \n",
    "# train a 1D convnet with global maxpoolinnb_wordsg  \n",
    "  \n",
    "#left model 第一块神经网络，卷积窗口是5*50（50是词向量维度）  \n",
    "model_left = Sequential()  \n",
    "#model.add(Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32'))  \n",
    "model_left.add(embedding_layer)  \n",
    "model_left.add(Conv1D(128, 5, activation='tanh'))  \n",
    "model_left.add(MaxPooling1D(5))  \n",
    "model_left.add(Conv1D(128, 5, activation='tanh'))  \n",
    "model_left.add(MaxPooling1D(5))  \n",
    "model_left.add(Conv1D(128, 5, activation='tanh'))  \n",
    "model_left.add(MaxPooling1D(35))  \n",
    "model_left.add(Flatten())  \n",
    "  \n",
    "#right model <span style=\"font-family:Arial, Helvetica, sans-serif;\">第二块神经网络，卷积窗口是4*50</span>  \n",
    "  \n",
    "model_right = Sequential()  \n",
    "model_right.add(embedding_layer)  \n",
    "model_right.add(Conv1D(128, 4, activation='tanh'))  \n",
    "model_right.add(MaxPooling1D(4))  \n",
    "model_right.add(Conv1D(128, 4, activation='tanh'))  \n",
    "model_right.add(MaxPooling1D(4))  \n",
    "model_right.add(Conv1D(128, 4, activation='tanh'))  \n",
    "model_right.add(MaxPooling1D(28))  \n",
    "model_right.add(Flatten())  \n",
    "  \n",
    "#third model <span style=\"font-family:Arial, Helvetica, sans-serif;\">第三块神经网络，卷积窗口是6*50</span>  \n",
    "model_3 = Sequential()  \n",
    "model_3.add(embedding_layer)  \n",
    "model_3.add(Conv1D(128, 6, activation='tanh'))  \n",
    "model_3.add(MaxPooling1D(3))  \n",
    "model_3.add(Conv1D(128, 6, activation='tanh'))  \n",
    "model_3.add(MaxPooling1D(3))  \n",
    "model_3.add(Conv1D(128, 6, activation='tanh'))  \n",
    "model_3.add(MaxPooling1D(30))  \n",
    "model_3.add(Flatten())  \n",
    "  \n",
    "  \n",
    "merged = Merge([model_left, model_right,model_3], mode='concat') # 将三种不同卷积窗口的卷积层组合 连接在一起，当然也可以只是用三个model中的一个，一样可以得到不错的效果，只是本文采用论文中的结构设计  \n",
    "model = Sequential()  \n",
    "model.add(merged) # add merge  \n",
    "model.add(Dense(128, activation='tanh')) # 全连接层  \n",
    "model.add(Dense(9, activation='softmax')) # softmax，输出文本属于20种类别中每个类别的概率  \n",
    "  \n",
    "# 优化器我这里用了adadelta，也可以使用其他方法  \n",
    "model.compile(loss='categorical_crossentropy',  \n",
    "              optimizer='Adadelta',  \n",
    "              metrics=['accuracy'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zheng\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py:848: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "54/54 [==============================] - 1s - loss: 2.1989 - acc: 0.0926     \n",
      "Epoch 2/100\n",
      "54/54 [==============================] - 0s - loss: 2.1941 - acc: 0.1481     \n",
      "Epoch 3/100\n",
      "54/54 [==============================] - 0s - loss: 2.1919 - acc: 0.2037     \n",
      "Epoch 4/100\n",
      "54/54 [==============================] - 0s - loss: 2.1902 - acc: 0.0741     \n",
      "Epoch 5/100\n",
      "54/54 [==============================] - 0s - loss: 2.1844 - acc: 0.1852     \n",
      "Epoch 6/100\n",
      "54/54 [==============================] - 0s - loss: 2.1821 - acc: 0.1481     \n",
      "Epoch 7/100\n",
      "54/54 [==============================] - 0s - loss: 2.1797 - acc: 0.1481     \n",
      "Epoch 8/100\n",
      "54/54 [==============================] - 0s - loss: 2.1746 - acc: 0.1852     \n",
      "Epoch 9/100\n",
      "54/54 [==============================] - 0s - loss: 2.1805 - acc: 0.1296     \n",
      "Epoch 10/100\n",
      "54/54 [==============================] - 0s - loss: 2.1733 - acc: 0.1481     \n",
      "Epoch 11/100\n",
      "54/54 [==============================] - 0s - loss: 2.1716 - acc: 0.1852     \n",
      "Epoch 12/100\n",
      "54/54 [==============================] - 0s - loss: 2.1719 - acc: 0.2037     \n",
      "Epoch 13/100\n",
      "54/54 [==============================] - 0s - loss: 2.1736 - acc: 0.1481     \n",
      "Epoch 14/100\n",
      "54/54 [==============================] - 0s - loss: 2.1681 - acc: 0.1296     \n",
      "Epoch 15/100\n",
      "54/54 [==============================] - 0s - loss: 2.1641 - acc: 0.1481     \n",
      "Epoch 16/100\n",
      "54/54 [==============================] - 0s - loss: 2.1691 - acc: 0.2037     \n",
      "Epoch 17/100\n",
      "54/54 [==============================] - 0s - loss: 2.1663 - acc: 0.1481     \n",
      "Epoch 18/100\n",
      "54/54 [==============================] - 0s - loss: 2.1648 - acc: 0.1296     \n",
      "Epoch 19/100\n",
      "54/54 [==============================] - 0s - loss: 2.1523 - acc: 0.1667     \n",
      "Epoch 20/100\n",
      "54/54 [==============================] - 0s - loss: 2.1498 - acc: 0.1481     \n",
      "Epoch 21/100\n",
      "54/54 [==============================] - 0s - loss: 2.1477 - acc: 0.2037     \n",
      "Epoch 22/100\n",
      "54/54 [==============================] - 0s - loss: 2.1413 - acc: 0.2407     \n",
      "Epoch 23/100\n",
      "54/54 [==============================] - 0s - loss: 2.1348 - acc: 0.2222     \n",
      "Epoch 24/100\n",
      "54/54 [==============================] - 0s - loss: 2.1290 - acc: 0.2407     \n",
      "Epoch 25/100\n",
      "54/54 [==============================] - 0s - loss: 2.1256 - acc: 0.2037     \n",
      "Epoch 26/100\n",
      "54/54 [==============================] - 0s - loss: 2.1060 - acc: 0.2222     \n",
      "Epoch 27/100\n",
      "54/54 [==============================] - 0s - loss: 2.0786 - acc: 0.2037     \n",
      "Epoch 28/100\n",
      "54/54 [==============================] - 0s - loss: 2.0541 - acc: 0.2593     \n",
      "Epoch 29/100\n",
      "54/54 [==============================] - 0s - loss: 2.0462 - acc: 0.1852     \n",
      "Epoch 30/100\n",
      "54/54 [==============================] - 0s - loss: 2.0052 - acc: 0.2407     \n",
      "Epoch 31/100\n",
      "54/54 [==============================] - 0s - loss: 1.9782 - acc: 0.2593     \n",
      "Epoch 32/100\n",
      "54/54 [==============================] - 0s - loss: 1.9439 - acc: 0.2407     \n",
      "Epoch 33/100\n",
      "54/54 [==============================] - 0s - loss: 1.9283 - acc: 0.2778     \n",
      "Epoch 34/100\n",
      "54/54 [==============================] - 0s - loss: 1.9100 - acc: 0.2593     \n",
      "Epoch 35/100\n",
      "54/54 [==============================] - 0s - loss: 1.9000 - acc: 0.2963     \n",
      "Epoch 36/100\n",
      "54/54 [==============================] - 0s - loss: 1.8722 - acc: 0.2593     \n",
      "Epoch 37/100\n",
      "54/54 [==============================] - 0s - loss: 1.8638 - acc: 0.2778     \n",
      "Epoch 38/100\n",
      "54/54 [==============================] - 0s - loss: 1.8364 - acc: 0.2037     \n",
      "Epoch 39/100\n",
      "54/54 [==============================] - 0s - loss: 1.8019 - acc: 0.3519     \n",
      "Epoch 40/100\n",
      "54/54 [==============================] - 0s - loss: 1.7813 - acc: 0.2778     \n",
      "Epoch 41/100\n",
      "54/54 [==============================] - 0s - loss: 1.7602 - acc: 0.3704     \n",
      "Epoch 42/100\n",
      "54/54 [==============================] - 0s - loss: 1.7380 - acc: 0.3704     \n",
      "Epoch 43/100\n",
      "54/54 [==============================] - 0s - loss: 1.6911 - acc: 0.3519     \n",
      "Epoch 44/100\n",
      "54/54 [==============================] - 0s - loss: 1.6405 - acc: 0.4259     \n",
      "Epoch 45/100\n",
      "54/54 [==============================] - 0s - loss: 1.6154 - acc: 0.4259     \n",
      "Epoch 46/100\n",
      "54/54 [==============================] - 0s - loss: 1.6200 - acc: 0.4074     \n",
      "Epoch 47/100\n",
      "54/54 [==============================] - 0s - loss: 1.5641 - acc: 0.3148     \n",
      "Epoch 48/100\n",
      "54/54 [==============================] - 0s - loss: 1.4773 - acc: 0.6296     \n",
      "Epoch 49/100\n",
      "54/54 [==============================] - 0s - loss: 1.4360 - acc: 0.5185     \n",
      "Epoch 50/100\n",
      "54/54 [==============================] - 0s - loss: 1.5674 - acc: 0.5556     \n",
      "Epoch 51/100\n",
      "54/54 [==============================] - 0s - loss: 1.4803 - acc: 0.3889     \n",
      "Epoch 52/100\n",
      "54/54 [==============================] - 0s - loss: 1.2819 - acc: 0.6111     \n",
      "Epoch 53/100\n",
      "54/54 [==============================] - 0s - loss: 1.2150 - acc: 0.6852     \n",
      "Epoch 54/100\n",
      "54/54 [==============================] - 0s - loss: 1.1212 - acc: 0.6296     \n",
      "Epoch 55/100\n",
      "54/54 [==============================] - 0s - loss: 1.0354 - acc: 0.7222     \n",
      "Epoch 56/100\n",
      "54/54 [==============================] - 0s - loss: 1.1042 - acc: 0.6667     \n",
      "Epoch 57/100\n",
      "54/54 [==============================] - 0s - loss: 0.9219 - acc: 0.7778     \n",
      "Epoch 58/100\n",
      "54/54 [==============================] - 0s - loss: 0.8905 - acc: 0.7222     \n",
      "Epoch 59/100\n",
      "54/54 [==============================] - 0s - loss: 0.9084 - acc: 0.6852     \n",
      "Epoch 60/100\n",
      "54/54 [==============================] - 0s - loss: 0.8043 - acc: 0.7778     \n",
      "Epoch 61/100\n",
      "54/54 [==============================] - 0s - loss: 0.6413 - acc: 0.8889     \n",
      "Epoch 62/100\n",
      "54/54 [==============================] - 0s - loss: 0.5512 - acc: 0.9074     \n",
      "Epoch 63/100\n",
      "54/54 [==============================] - 0s - loss: 0.4969 - acc: 0.8333     \n",
      "Epoch 64/100\n",
      "54/54 [==============================] - 0s - loss: 0.4280 - acc: 0.9259     \n",
      "Epoch 65/100\n",
      "54/54 [==============================] - 0s - loss: 0.4241 - acc: 0.9074     \n",
      "Epoch 66/100\n",
      "54/54 [==============================] - 0s - loss: 0.5182 - acc: 0.8148     \n",
      "Epoch 67/100\n",
      "54/54 [==============================] - 0s - loss: 0.4121 - acc: 0.8333     \n",
      "Epoch 68/100\n",
      "54/54 [==============================] - 0s - loss: 0.3120 - acc: 0.9259     \n",
      "Epoch 69/100\n",
      "54/54 [==============================] - 0s - loss: 0.3397 - acc: 0.9074     \n",
      "Epoch 70/100\n",
      "54/54 [==============================] - 0s - loss: 0.3145 - acc: 0.9074     \n",
      "Epoch 71/100\n",
      "54/54 [==============================] - 0s - loss: 0.2257 - acc: 0.9444     \n",
      "Epoch 72/100\n",
      "54/54 [==============================] - 0s - loss: 0.1842 - acc: 1.0000     \n",
      "Epoch 73/100\n",
      "54/54 [==============================] - 0s - loss: 0.1349 - acc: 1.0000     \n",
      "Epoch 74/100\n",
      "54/54 [==============================] - 0s - loss: 0.1218 - acc: 1.0000     \n",
      "Epoch 75/100\n",
      "54/54 [==============================] - 0s - loss: 0.1134 - acc: 1.0000     \n",
      "Epoch 76/100\n",
      "54/54 [==============================] - 0s - loss: 0.0970 - acc: 1.0000     \n",
      "Epoch 77/100\n",
      "54/54 [==============================] - 0s - loss: 0.0910 - acc: 1.0000     \n",
      "Epoch 78/100\n",
      "54/54 [==============================] - 0s - loss: 0.0807 - acc: 1.0000     \n",
      "Epoch 79/100\n",
      "54/54 [==============================] - 0s - loss: 0.0774 - acc: 1.0000     \n",
      "Epoch 80/100\n",
      "54/54 [==============================] - 0s - loss: 0.0637 - acc: 1.0000     \n",
      "Epoch 81/100\n",
      "54/54 [==============================] - 0s - loss: 0.0590 - acc: 1.0000     \n",
      "Epoch 82/100\n",
      "54/54 [==============================] - 0s - loss: 0.0537 - acc: 1.0000     \n",
      "Epoch 83/100\n",
      "54/54 [==============================] - 0s - loss: 0.0519 - acc: 1.0000     \n",
      "Epoch 84/100\n",
      "54/54 [==============================] - 0s - loss: 0.0471 - acc: 1.0000     \n",
      "Epoch 85/100\n",
      "54/54 [==============================] - 0s - loss: 0.0475 - acc: 1.0000     \n",
      "Epoch 86/100\n",
      "54/54 [==============================] - 0s - loss: 0.0452 - acc: 1.0000     \n",
      "Epoch 87/100\n",
      "54/54 [==============================] - 0s - loss: 0.0380 - acc: 1.0000     \n",
      "Epoch 88/100\n",
      "54/54 [==============================] - 0s - loss: 0.0352 - acc: 1.0000     \n",
      "Epoch 89/100\n",
      "54/54 [==============================] - 0s - loss: 0.0335 - acc: 1.0000     \n",
      "Epoch 90/100\n",
      "54/54 [==============================] - 0s - loss: 0.0312 - acc: 1.0000     \n",
      "Epoch 91/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 [==============================] - 0s - loss: 0.0294 - acc: 1.0000     \n",
      "Epoch 92/100\n",
      "54/54 [==============================] - 0s - loss: 0.0279 - acc: 1.0000     \n",
      "Epoch 93/100\n",
      "54/54 [==============================] - 0s - loss: 0.0266 - acc: 1.0000     \n",
      "Epoch 94/100\n",
      "54/54 [==============================] - 0s - loss: 0.0257 - acc: 1.0000     \n",
      "Epoch 95/100\n",
      "54/54 [==============================] - 0s - loss: 0.0234 - acc: 1.0000     \n",
      "Epoch 96/100\n",
      "54/54 [==============================] - 0s - loss: 0.0220 - acc: 1.0000     \n",
      "Epoch 97/100\n",
      "54/54 [==============================] - 0s - loss: 0.0210 - acc: 1.0000     \n",
      "Epoch 98/100\n",
      "54/54 [==============================] - 0s - loss: 0.0206 - acc: 1.0000     \n",
      "Epoch 99/100\n",
      "54/54 [==============================] - 0s - loss: 0.0197 - acc: 1.0000     \n",
      "Epoch 100/100\n",
      "54/54 [==============================] - 0s - loss: 0.0184 - acc: 1.0000     \n",
      "32/36 [=========================>....] - ETA: 0s0.361111111111\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, nb_epoch=100)  \n",
    "# score = model.evaluate(x_train, y_train, verbose=0) # 评估模型在训练集中的效果，准确率约99%  \n",
    "# print('train score:', score[0])  \n",
    "# print('train accuracy:', score[1])  \n",
    "score, acc = model.evaluate(X_test, y_test, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
